{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIML Predictions\n",
    "\n",
    "In this notebook we train and evaluate CIML experiments using the functions `gather_results` and `tf_trainer functions` of the [ciml project](https://github.com/mtreinish/ciml). \n",
    "<br>Then we save the predictions of the experiments for a deeper analysis of the metrics of the trained models (see [CIML Metric Report](https://nbviewer.jupyter.org/github/kwulffert/ciml_experiments/blob/master/Metrics%20report.ipynb))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ciml import gather_results\n",
    "from ciml import tf_trainer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.training import adagrad\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the data path, dataset and experiment to gather the right input dataset and the configuration for the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/kw/ciml_data/cimlodsceu2019seed'\n",
    "\n",
    "#Dataset and experiment combination for multiple classification\n",
    "#dataset = 'usr_1m-1min-node_provider'\n",
    "#experiment = 'dnn-3x100-500epochs-bs128'\n",
    "\n",
    "#Dataset and experiment combination for binary classification\n",
    "dataset = 'usr_1m-1min-status'\n",
    "experiment = 'dnn-5x100-500epochs-bs128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_data = gather_results.load_experiment(\n",
    "        experiment, data_path=data_path)\n",
    "dataset_data = gather_results.load_model_config(\n",
    "        dataset, data_path=data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "The dataset and experiment_data are dictionaries with the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['build_name', 'sample_interval', 'features_regex', 'class_label', 'aggregation_functions', 'training_set', 'dev_set', 'test_set', 'normalized_length', 'labels', 'num_columns', 'num_features', 'normalization_params'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'estimator': 'tf.estimator.DNNClassifier',\n",
       " 'params': {},\n",
       " 'hyper_params': {'steps': 9500,\n",
       "  'batch_size': '128',\n",
       "  'epochs': '500',\n",
       "  'hidden_units': [100, 100, 100, 100, 100],\n",
       "  'optimizer': 'Adagrad',\n",
       "  'learning_rate': 0.05}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set up the experiment and configure the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = experiment_data['estimator']\n",
    "hyper_params = experiment_data['hyper_params']\n",
    "params = experiment_data['params']\n",
    "steps = int(hyper_params['steps'])\n",
    "num_epochs = int(hyper_params['epochs'])\n",
    "batch_size = int(hyper_params['batch_size'])\n",
    "optimizer = hyper_params['optimizer']\n",
    "learning_rate = float(hyper_params['learning_rate'])\n",
    "class_label = dataset_data['class_label']\n",
    "labels = gather_results.load_dataset(dataset, 'labels', data_path=data_path)['labels']\n",
    "training_data = gather_results.load_dataset(dataset, 'training', data_path=data_path)\n",
    "test_data = gather_results.load_dataset(dataset, 'test', data_path=data_path)\n",
    "\n",
    "#label_vocabulary = None\n",
    "if class_label == 'node_provider':\n",
    "    label_vocabulary = set(['rax', 'ovh', 'packethost-us-west-1',\n",
    "                            'vexxhost', 'limestone-regionone',\n",
    "                            'inap-mtl01', 'fortnebula-regionone'])\n",
    "elif class_label == 'node_provider_all':\n",
    "    label_vocabulary = set(['rax-iad', 'ovh-bhs1', 'packethost-us-west-1',\n",
    "                            'rax-dfw', 'vexxhost-ca-ymq-1', 'ovh-gra1',\n",
    "                            'limestone-regionone', 'inap-mtl01', 'rax-ord',\n",
    "                            'vexxhost-sjc1', 'fortnebula-regionone'])\n",
    "else:\n",
    "    label_vocabulary = None\n",
    "\n",
    "model_dir = gather_results.get_model_folder(dataset, experiment, data_path=data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf_trainer.get_estimator(\n",
    "        estimator, hyper_params, params, labels, model_dir,\n",
    "        optimizer=adagrad.AdagradOptimizer(learning_rate=0.05),\n",
    "        label_vocabulary=label_vocabulary, gpu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fn=tf_trainer.get_input_fn(shuffle=True,\n",
    "                    batch_size=batch_size, num_epochs=num_epochs,\n",
    "                    labels=labels, **training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_result = tf_trainer.get_training_method(estimator)(\n",
    "                    input_fn=tf_trainer.get_input_fn(shuffle=True,\n",
    "                    batch_size=batch_size, num_epochs=num_epochs,\n",
    "                    labels=labels, **training_data), steps=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the trained model with the testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = gather_results.load_dataset(dataset, 'test', data_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['examples', 'example_ids', 'classes'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_size = len(eval_data['example_ids'])\n",
    "eval_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyse the predictions of our trained model.\n",
    "<br>Info logging is enabled to monitor the status of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = estimator.predict(input_fn=tf_trainer.get_input_fn(\n",
    "                                batch_size=eval_size, num_epochs=1,\n",
    "                                labels=labels, **eval_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [x for x in prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[744, 37, 7, 6]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_classes = [x[\"class_ids\"][0] for x in predictions]\n",
    "actual_classes = eval_data[\"classes\"]\n",
    "classes = zip(p_classes, actual_classes)\n",
    "counter = collections.Counter(classes)\n",
    "sorted(counter.values(), reverse=True)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(0, 0): 744, (1, 1): 37, (0, 1): 7, (1, 0): 6})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the predictions of the trained model in a zipped json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "serializable_pred = []\n",
    "for pred in predictions:\n",
    "    _classes = pred['classes']\n",
    "    _all_classes = pred['all_classes']\n",
    "    pred['classes'] = [x.decode(\"utf-8\") for x in _classes]\n",
    "    pred['all_classes'] = [x.decode(\"utf-8\") for x in _all_classes]\n",
    "    serializable_pred.append(pred)\n",
    "\n",
    "prediction_name = \"prediction_\" + dataset\n",
    "pred_data = zip(eval_data['example_ids'], serializable_pred,\n",
    "                eval_data['classes'])\n",
    "gather_results.save_data_json(\n",
    "    dataset, [x for x in pred_data],\n",
    "    prediction_name, sub_folder=experiment, data_path=data_path)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": ".venv-mpl",
   "language": "python",
   "name": ".venv-mpl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
